{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from sacremoses) (2024.9.11)\n",
      "Requirement already satisfied: click in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from sacremoses) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Requirement already satisfied: nltk in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: rouge in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\yibor\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses\n",
    "!pip install -U nltk\n",
    "!pip install rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(path):\n",
    "    data_file = os.path.join(path)\n",
    "    with open(data_file,\"r\") as f:\n",
    "        lang_data = f.read()\n",
    "    return lang_data.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = load_data('data/small_vocab_en.txt')\n",
    "# Load French data\n",
    "french_sentences = load_data('data/small_vocab_fr.txt')\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137861\n",
      "137861\n"
     ]
    }
   ],
   "source": [
    "print(len(english_sentences))\n",
    "print(len(french_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"Tokenize a list of sentences using a pre-trained MarianMT tokenizer.\"\"\"\n",
    "    return tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize the English and French sentences\n",
    "tokenized_english = tokenize_sentences(english_sentences).to(device)\n",
    "tokenized_french = tokenize_sentences(french_sentences).to(device)\n",
    "# print(tokenized_english['input_ids'].shape) \n",
    "# print(tokenized_french['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  191, 44431,    32,  ..., 59513, 59513, 59513],\n",
      "        [    4, 19710,  2667,  ..., 59513, 59513, 59513],\n",
      "        [ 7418,   263,  2636,  ..., 59513, 59513, 59513],\n",
      "        ...,\n",
      "        [12502,    51,    32,  ..., 59513, 59513, 59513],\n",
      "        [    4, 12610,    32,  ..., 59513, 59513, 59513],\n",
      "        [    0, 59513, 59513,  ..., 59513, 59513, 59513]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[  191, 44431,    43,  ..., 59513, 59513, 59513],\n",
      "        [   16,    49,  2505,  ..., 59513, 59513, 59513],\n",
      "        [ 7418,   263,  2636,  ..., 59513, 59513, 59513],\n",
      "        ...,\n",
      "        [    8, 12502,    51,  ..., 59513, 59513, 59513],\n",
      "        [   14,     6,   247,  ..., 59513, 59513, 59513],\n",
      "        [    0, 59513, 59513,  ..., 59513, 59513, 59513]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_english)\n",
    "print(tokenized_french)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3722247 English tokens.\n",
      "221 unique English tokens.\n",
      "10 Most common tokens in the English dataset:\n",
      "\"59513\" \"32\" \"49\" \"2\" \"0\" \"250\" \"18\" \"61\" \"475\" \"4\"\n",
      "\n",
      "6479467 French tokens.\n",
      "386 unique French tokens.\n",
      "10 Most common tokens in the French dataset:\n",
      "\"59513\" \"43\" \"49\" \"9\" \"0\" \"250\" \"2\" \"23\" \"92\" \"1101\"\n"
     ]
    }
   ],
   "source": [
    "# Print the tokenized output\n",
    "# print(\"Tokenized English input IDs:\", tokenized_english['input_ids'])\n",
    "# print(\"Tokenized French input IDs:\", tokenized_french['input_ids'])\n",
    "english_tokens_counter = collections.Counter(token for sentence in tokenized_english['input_ids'].cpu() for token in sentence.numpy())\n",
    "french_tokens_counter = collections.Counter(token for sentence in tokenized_french['input_ids'].cpu() for token in sentence.numpy())\n",
    "\n",
    "print('{} English tokens.'.format(len([token for sentence in tokenized_english['input_ids'].cpu() for token in sentence.numpy()])))\n",
    "print('{} unique English tokens.'.format(len(english_tokens_counter)))\n",
    "print('10 Most common tokens in the English dataset:')\n",
    "# print('\"' + '\" \"'.join(list(zip(*english_tokens_counter.most_common(10)))[0]) + '\"')\n",
    "counts = list(zip(*english_tokens_counter.most_common(10)))[0]  # Extract the counts\n",
    "print('\"' + '\" \"'.join(map(str, counts)) + '\"')\n",
    "# num_unique_english_tokens = len(english_tokens_counter)\n",
    "# input_size = num_unique_english_tokens\n",
    "# print(\"Input size\",input_size)\n",
    "print()\n",
    "print('{} French tokens.'.format(len([token for sentence in tokenized_french['input_ids'].cpu() for token in sentence.numpy()])))\n",
    "print('{} unique French tokens.'.format(len(french_tokens_counter)))\n",
    "print('10 Most common tokens in the French dataset:')\n",
    "counts = list(zip(*french_tokens_counter.most_common(10)))[0]  # Extract the counts\n",
    "print('\"' + '\" \"'.join(map(str, counts)) + '\"')\n",
    "# num_unique_french_tokens = len(french_tokens_counter)\n",
    "\n",
    "# output_size = num_unique_french_tokens\n",
    "# print(\"Output size\",output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello yassine is the best student Morocco is here', 'califor']\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "# Load the MarianMT tokenizer (English to French)\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
    "\n",
    "# Example token IDs\n",
    "token_ids = [\n",
    "    [34536,  6236,     9,     9,  1054,    32,     4,   877,  6548,  8957,\n",
    "            32,   479,     0],  # Example token IDs\n",
    "    [7418, 263, 2636, 59513, 59513]  # Another example\n",
    "]\n",
    "\n",
    "# Decode the token IDs into readable text\n",
    "decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in token_ids]\n",
    "print(decoded_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('configs/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Loaded\n",
      "Input Size:  59514\n",
      "Output Size:  59514\n",
      "Hidden Size:  64\n",
      "Number of Layers:  2\n"
     ]
    }
   ],
   "source": [
    "input_size = tokenizer.vocab_size\n",
    "hidden_size = config['model']['hidden_size']\n",
    "num_layers = config['model']['num_layers']\n",
    "output_size = tokenizer.vocab_size\n",
    "print('Config Loaded')\n",
    "print('Input Size: ', input_size)\n",
    "print('Output Size: ', output_size)\n",
    "print('Hidden Size: ', hidden_size)\n",
    "print('Number of Layers: ', num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StackedRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, output_size, num_layers=2):\n",
    "        super(StackedRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer: maps vocab indices to embedding vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # Embed input word indices to word vectors\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Pass through RNN layer\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshape output for the fully connected layer\n",
    "        x = x.contiguous().view(-1, self.hidden_size)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, hidden\n",
    "    \n",
    "    # Helper method to initialize hidden state\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Return a zero-initialized hidden state for RNN (num_layers, batch_size, hidden_size)\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "# Example initialization\n",
    "model_rnn = StackedRNN(vocab_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through stacked LSTM\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return nn.functional.log_softmax(out, dim=2)\n",
    "\n",
    "# Example usage\n",
    "model_lstm = StackedLSTM(input_size, hidden_size, output_size, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(StackedGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through stacked GRU\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out)\n",
    "        return nn.functional.log_softmax(out, dim=2)\n",
    "\n",
    "# Example usage\n",
    "model_gru = StackedGRU(input_size, hidden_size, output_size, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenized_english, tokenized_french, num_epochs, batch_size, learning_rate, device):\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    # Initialize the loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    rouge_scores = Rouge()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(tokenized_english['input_ids']), batch_size):\n",
    "            # Get batch inputs and targets\n",
    "            input_ids = tokenized_english['input_ids'][i:i + batch_size].to(device)\n",
    "            target_ids = tokenized_french['input_ids'][i:i + batch_size].to(device)\n",
    "            hidden = model.init_hidden(input_ids.size(0), device)\n",
    "\n",
    "            # Forward pass\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Unpack outputs and hidden from the model\n",
    "            outputs, hidden = model(input_ids, hidden)\n",
    "\n",
    "            # Reshape target and outputs for loss computation\n",
    "            target_ids = target_ids.view(-1)  # Flatten target tensor\n",
    "            outputs = outputs.view(-1, outputs.size(-1))  # Flatten outputs tensor\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, target_ids)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate metrics after each epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_sentences = []  # Store generated sentences for BLEU and METEOR\n",
    "            reference_sentences = []  # Store reference sentences\n",
    "            \n",
    "            for input_id in tokenized_english['input_ids']:\n",
    "                # Forward pass to get the model output, ignore hidden\n",
    "                output, _ = model(input_id.unsqueeze(0).to(device), hidden=None)  \n",
    "                predicted_ids = torch.argmax(output, dim=-1)\n",
    "                generated_sentences.append(predicted_ids.cpu())  # Detach and append predictions\n",
    "                reference_sentences.append(target_ids)  # Collect reference sentences\n",
    "\n",
    "            # Calculate BLEU score\n",
    "            bleu = corpus_bleu(reference_sentences, generated_sentences)\n",
    "            bleu_scores.append(bleu)\n",
    "            \n",
    "            # Calculate ROUGE score\n",
    "            rouge_score = rouge_scores.get_scores(generated_sentences, reference_sentences, avg=True)\n",
    "            rouge_scores.append(rouge_score)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, BLEU: {bleu:.4f}, ROUGE: {rouge_score:.4f}')\n",
    "\n",
    "    return bleu_scores, meteor_scores, rouge_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = config['training']['num_epochs']\n",
    "batch_size = config['training']['batch_size']\n",
    "learning_rate = config['training']['learning_rate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (864) to match target batch_size (1504).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m [model_rnn, model_lstm, model_gru]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_english\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_french\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained successfully!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[65], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, tokenized_english, tokenized_french, num_epochs, batch_size, learning_rate, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Flatten outputs tensor\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\yibor\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yibor\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yibor\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:218\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yibor\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:2778\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2777\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (864) to match target batch_size (1504)."
     ]
    }
   ],
   "source": [
    "models = [model_rnn, model_lstm, model_gru]\n",
    "for model in models:\n",
    "    train(model, tokenized_english, tokenized_french, num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate,device=device)\n",
    "    print(model.__class__.__name__, 'trained successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
